Public and private bodies are continuously seeking for new analytical tools and methods to assess, rank and compare their performance based 
on different indicators and dimensions with the objective of making some decision or developing a new policy. 
In this context the creation and use of quantitative indexes is a wider practice that has been applied to various 
domains such as bibliometrics and academic performance and quality (the Impact Factor by Thomson-Reuters or the H-index), 
the Web impact (the Webindex by the Webfoundation) or Cloud Computing (the CSMIC index, the Global Cloud Index by Cisco, the CSC index, 
the VMWare Cloud Index, etc.) or university quality (the Shanghai or the Webometrics rankings) to name a few. 
Therefore policy-makers as well as individuals are continuously evaluating quantitative measures to tackle 
existing problems and support their decisions. Nevertheless the sheer mass of data now available in the web is 
raising a new dynamic and challenging environment in which traditional tools are facing major 
problems to deal with datasources diversity, structural issues or complex processes of estimation. Following some efforts 
such as the ``Policy-making $2.0$'' within the Cross-Over project that \textit{refers to a blend of emerging and fast developing technologies 
that enable better, more timely and more participated decision-making}, new paradigms and tools are required to take advantage of 
the existing environment (open data and big data) to design and estimate actions in this dynamic environment according to requirements of 
transparency, standardization, adaptability and extensibility among others with the aim of providing new context-aware 
and added-value services such as visualization that can help a deepen and broaden understanding of the impact of a 
policy in a more fast and efficient way. As a consequence common features and requirements can be extracted out from the existing situation:
\begin{itemize}
 \item Data sources. Data and information is continuously being generated as observations from social networks, public and private institutions, NGOs, services and applications, etc. 
 creating a tangled environment of sources, formats and access protocols with a huge potential for exploitation. Nevertheless data processing, knowledge inferring, etc. are not mere processes 
 of retrieving and gathering, it is necessary to deal with semantic and syntactic issues with the aim of enabling a proper re-use of data and information 
 and generate new knowledge. 
 
 \item Structure. Quantitative indexes are usually defined (a mathematical model) by experts to aggregate several indicators (in a tree structure) in just one value to provide
 a measure of the impact or performance of some policy in a certain context. The structure of these indexes are obviously subjected to change over time 
 to collect more information or adjust some parameters. That is why technology should be able to afford proper techniques 
 to automatically represent new changes in an efficient way.
 
  \item Computation process. This feature refers to the calculation of the index. Observations are gathered from different data sources and aligned 
  to the index structure, commonly indicators, that are processed through different mathematical operators to populate a final index value. 
  Nevertheless the computation process is not always open (any minor change can imply a long time for validation) and can not be replied for third-parties with other purposes, for instance research, preventing some 
  of most wanted characteristics such as transparency. Furhermore it is not possible to assess if the index is going to be or has been computed in 
  the right way.
  
  \item Documentation. As the European project Cross-over has stated, new policy-making strategies go ahead of a simple and closed value and it is necessary to provide 
  new ways of exploiting data and information. Moreover the use of the Web as a dissemination channel represents a powerful environment in which 
  information should be available taking into account multilingual and multicultural character of information. In this context documentation mechanisms 
  must necessarily cover all the aforementioned features to afford a detailed explaination of a quantitative index-based policy to both policy-makers 
  and final users but existing techniques usually generates some kind of hand-made report which is not easy to keep up-to-date and deliver 
  to the long-tail of interested third-parties.
\end{itemize}

On the other hand, the Semantic Web area has experienced during last years a growing commitment from both academia and industrial areas 
with the objective of elevating the meaning of web information resources through a common and shared data model (graphs) and 
an underlying semantics based on different logic formalisms (ontologies). The Resource Description Framework (RDF), based on a graph model, 
and the Web Ontology Language (OWL), designed to formalize and model domain knowledge, are a \textit{lingua-franca} to reuse information 
and data in a knowledge-based environment. Thus data, information and knowledge can be easily shared, exchanged and linked~\cite{Maali_Cyganiak_2011} 
to other knowledge and databases through the use URIs, more specifically HTTP-URIs. Therefore the broad objective of this effort can 
be summarized as a new environment of data-based services that can encourage and improve B2B (Business to Business), B2C (Business to Client) or 
A2A (Administration to Administration) relationships by means of the implementation of new contex-awareness expert systems 
to tackle existing cross-domain problems in which data heterogeneities, lack of standard knowledge representation and 
interoperability problems are common factors. As part of the Semantic Web area, recent times have also seen the deployment of 
the Linked (Open) Data initiative  to make it possible the view and application of the Semantic Web to create a large 
and distributed database on the Web. 

Obviously semantic web technologies can partially fulfill the features and requirements of this challenging environment for supporting 
new policy-making strategies. A common and shared data model based on existing standardized semantic web vocabularies and datasets can be used to 
represent quantitative indexes from both, structural and computational, points of view enabling the right exploitation of metadata and semantics. 
That is why the present paper introduces: 1) a high-level model on top of the RDF Data Cube Vocabulary~\cite{rdf-data-cube}, a shared effort to model statistical data in RDF reusing parts 
(the cube model) of the Statistical Data and Metadata Exchange Vocabulary~\cite{sdmx} (SDMX), to represent the structure of quantitative indexes and 
2) a Java-SPARQL based processor to exploit the metainformation and compute new values of an index.

Finally, as a motivating and on-going example, see Table~\ref{tab:example-wb}, a policy-maker wants to re-use the WorldBank data to model and compute a new index, ``The Naive World Bank Index''. This 
index uses the topics ``Aid Efectiveness'' ($c_1$) and ``Health'' ($c_2$) with the aim of comparing the status and health evolution in several countries to decide whether new 
investments are performed. From these components two indicators has been respectively selected by experts: ``Life Expectancy'' ($in_1$) and ``Health expenditure, total (\%) of GDP'' ($in_2$). 
Once components and their indicators are defined and data can be retrieved from the WorldBank it is necessary to set how the index and components are computed 
taking into account that only data about indicators is available. Following a top-down approach the index, $i$, is calculated through an ordered weighted averaging (OWA) operator using the 
formula: $\sum_{i=1}^n  w_i c_i$, where $w_i$ is the weight of the component $c_i$. On the other hand, both components only aggregates one indicator but the ``Aid Efectiveness'' 
must firstly compute the ``Life Expectancy'' without considering the sex dimension. Making this implies the need of populating the average age by country and year to create 
a new ``derivated'' indicator of ``Life Expectancy''. Apart from that the computation must also consider the observation status and values must be normalized using the 
z-score before computing intermediate and final values for each indicator, component and index. Furthermore this index is supposed to change in the future 
adding new data sources and metadata, modifying the computation processes (weights) or the structure (new components, indicators and dimensions). Finally, the policy-maker 
is also interested in applying this index to other scenarios and he also needs a way of explaining in different languages how the index is computed. 


\begin{table}[!htb]
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|p{3cm}||p{4cm}|p{1.8cm}|p{1.8cm}|p{1cm}|}
\hline
  \textbf{Component} & \textbf{Indicator} & \textbf{Year} & \textbf{Country} & \textbf{Value}  \\  \hline
  Aid Efectiveness & Life Expectancy Male & 2010 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2011 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2010 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2011 & Spain & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2010 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Male & 2011 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2010 & Greece & $1.0$ \\ \hline
  Aid Efectiveness & Life Expectancy Female & 2011 & Greece & $1.0$ \\ \hline
  Health & Health expenditure, total (\% of GDP) & 2010 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\% of GDP) & 2011 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\% of GDP) & 2010 & Spain & $1.0$ \\ \hline
  Health & Health expenditure, total (\% of GDP) & 2011 & Spain & $1.0$ \\ \hline
  \hline
  \end{tabular}
  \caption{Example of observations from the WorldBank.}
  \label{tab:example-wb}
  \end{center}
\end{table} 

